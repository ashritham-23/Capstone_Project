{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2947d281-1300-45ee-aa8e-742ccf5e61f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "SILVER LAYER – CLEANING & TRANSFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbf73451-3606-4acc-8acf-2e571a289cce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.empty-table+json": {
       "directive_name": "NoDirective"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "USE CATALOG main;\n",
    "USE SCHEMA agriculture_data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c5b7508-5688-4290-8f0e-76a3f7896554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Bronze Crop Production Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "594cd60b-fe8c-4e02-bba2-6ab8466eb735",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load raw crop production data from bronze layer\n",
    "crop_bronze_df = spark.table(\"bronze_crop_production\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "811a0a67-a42b-4380-b5b3-4d6ca718ad72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Validate and Clean Crop Production Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "572dedd0-c0cc-4564-9457-2e33679cffee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim, lower, avg, col, count, sum, max, min, lit, when, round, isnull\n",
    "\n",
    "# Apply core data quality rules\n",
    "\n",
    "crop_clean_df = (\n",
    "    crop_bronze_df\n",
    "    # Area cultivated must be greater than zero\n",
    "    .filter(col(\"area_hectares\") > 0)\n",
    "    # Production should not be null\n",
    "    .filter(col(\"production_tonnes\").isNotNull())\n",
    "    # Mandatory dimension fields should not be null\n",
    "    .filter(\n",
    "        col(\"state\").isNotNull() &\n",
    "        col(\"district\").isNotNull() &\n",
    "        col(\"crop\").isNotNull() &\n",
    "        col(\"year\").isNotNull()\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86b59bde-4c01-4f09-8fa4-e42211b4f0d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 10000\nRecords with missing keys: 0\n"
     ]
    }
   ],
   "source": [
    "# Data Completeness Check\n",
    "\n",
    "# Count total records after cleaning\n",
    "total_records = crop_clean_df.count()\n",
    "\n",
    "# Count records with missing key fields\n",
    "null_key_records = crop_clean_df.filter(\n",
    "    col(\"state\").isNull() |\n",
    "    col(\"district\").isNull() |\n",
    "    col(\"crop\").isNull() |\n",
    "    col(\"year\").isNull()\n",
    ").count()\n",
    "\n",
    "# Print total record count\n",
    "print(f\"Total records: {total_records}\")\n",
    "# Print records with missing keys\n",
    "print(f\"Records with missing keys: {null_key_records}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e034c615-b097-475f-b3de-40c333825df1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Standardize Text Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87edf4dc-2012-45e1-b783-28e0535a7d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Standardize text columns to avoid duplicates\n",
    "crop_standardized_df = (\n",
    "    crop_clean_df\n",
    "    # Normalizing  values\n",
    "    .withColumn(\"state\", trim(lower(col(\"state\"))))\n",
    "    .withColumn(\"district\", trim(lower(col(\"district\"))))\n",
    "    .withColumn(\"crop\", trim(lower(col(\"crop\"))))\n",
    "    .withColumn(\"season\", trim(lower(col(\"season\"))))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abc9a18f-38de-495a-8a3e-b8817602829d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Derive Yield Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eb5f852-b3fe-41ac-a9da-bb4dae0063a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid yield records (<=0): 0\n"
     ]
    }
   ],
   "source": [
    "# Calculate yield as production per hectare\n",
    "crop_enriched_df = (\n",
    "    crop_standardized_df\n",
    "    .withColumn(\n",
    "        \"yield_tonnes_per_hectare\",\n",
    "        col(\"production_tonnes\") / col(\"area_hectares\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Yield Validation\n",
    "\n",
    "# Count invalid yield values\n",
    "invalid_yield_records = crop_enriched_df.filter(\n",
    "    col(\"yield_tonnes_per_hectare\") <= 0\n",
    ").count()\n",
    "\n",
    "# Print invalid yield count\n",
    "print(f\"Invalid yield records (<=0): {invalid_yield_records}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6704c4d-3cc7-4039-a18f-5bf692826686",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Silver – Logging – crop_production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "369604c2-6b61-434e-b0d7-c15929ff2e26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, LongType, TimestampType\n",
    ")\n",
    "from pyspark.sql.functions import current_timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60f4620d-942f-4937-82c5-5ed244850faa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "log_schema = StructType([\n",
    "    StructField(\"pipeline_name\", StringType(), True),\n",
    "    StructField(\"layer\", StringType(), True),\n",
    "    StructField(\"table_name\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"record_count\", LongType(), True),\n",
    "    StructField(\"start_time\", TimestampType(), True),\n",
    "    StructField(\"end_time\", TimestampType(), True),\n",
    "    StructField(\"error_message\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db022069-ca3b-4e92-80bd-5a458f493bd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Logging metadata\n",
    "pipeline_name = \"agriculture_pipeline\"\n",
    "layer = \"SILVER\"\n",
    "table_name = \"silver_crop_production\"\n",
    "\n",
    "try:\n",
    "    # Record count after silver transformation\n",
    "    record_count = crop_enriched_df.count()\n",
    "\n",
    "    # Write silver table\n",
    "    crop_enriched_df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(table_name)\n",
    "\n",
    "    # Create success log dataframe\n",
    "    log_df = spark.createDataFrame(\n",
    "        [(pipeline_name, layer, table_name, \"SUCCESS\", record_count, None, None, None)],\n",
    "        schema=log_schema\n",
    "    )\n",
    "\n",
    "    # Add timestamps\n",
    "    log_df = log_df \\\n",
    "        .withColumn(\"start_time\", current_timestamp()) \\\n",
    "        .withColumn(\"end_time\", current_timestamp())\n",
    "\n",
    "    # Append log entry with schema merge enabled\n",
    "    log_df.write.format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(\"agriculture_data.pipeline_logs\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Create failure log dataframe\n",
    "    log_df = spark.createDataFrame(\n",
    "        [(pipeline_name, layer, table_name, \"FAILED\", None, None, None, str(e))],\n",
    "        schema=log_schema\n",
    "    )\n",
    "\n",
    "    # Add timestamps\n",
    "    log_df = log_df \\\n",
    "        .withColumn(\"start_time\", current_timestamp()) \\\n",
    "        .withColumn(\"end_time\", current_timestamp())\n",
    "\n",
    "    # Append failure log\n",
    "    log_df.write.format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(\"agriculture_data.pipeline_logs\")\n",
    "\n",
    "    # Fail task for Airflow\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb9b1671-737f-4e83-b05e-04cb2d9bfbf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Bronze Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e923f715-ea89-4451-88fc-0c542be06341",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load raw weather data\n",
    "weather_bronze_df = spark.table(\"bronze_weather\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f27e71b-f4d6-4cd6-bd61-5f68ba6ba47a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Clean and Validate Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a63bbebf-afcd-423f-b43f-2bafad9b716c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Apply weather data validation rules\n",
    "weather_clean_df = (\n",
    "    weather_bronze_df\n",
    "    # Valid rainfall values\n",
    "    .filter(col(\"rainfall_mm\") > 0)\n",
    "    # Mandatory dimension fields should not be null\n",
    "    .filter(\n",
    "        col(\"state\").isNotNull() &\n",
    "        col(\"district\").isNotNull() &\n",
    "        col(\"year\").isNotNull()\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d5cc3bf-1407-4e1e-b4cf-caf2207be7af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Handle Duplicate Weather Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aaedbd6-1edc-4013-8891-6c9b271267cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate rainfall by region and year\n",
    "weather_aggregated_df = (\n",
    "    weather_clean_df\n",
    "    .groupBy(\"state\", \"district\", \"year\")\n",
    "    .agg(avg(\"rainfall_mm\").alias(\"avg_rainfall_mm\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8df3cc66-e383-4542-8204-5dd2d9116230",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Silver – Logging – weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a87189a7-b438-4fc3-80fe-01b2e2b8f804",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Logging metadata\n",
    "pipeline_name = \"agriculture_pipeline\"\n",
    "layer = \"SILVER\"\n",
    "table_name = \"silver_weather\"\n",
    "\n",
    "try:\n",
    "    # Record count after aggregation\n",
    "    record_count = weather_aggregated_df.count()\n",
    "\n",
    "    # Write silver weather table\n",
    "    weather_aggregated_df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(table_name)\n",
    "\n",
    "    # Create success log dataframe\n",
    "    log_df = spark.createDataFrame(\n",
    "        [(pipeline_name, layer, table_name, \"SUCCESS\", record_count, None, None, None)],\n",
    "        schema=log_schema\n",
    "    )\n",
    "\n",
    "    # Add timestamps\n",
    "    log_df = log_df.withColumn(\"start_time\", current_timestamp()) \\\n",
    "                   .withColumn(\"end_time\", current_timestamp())\n",
    "\n",
    "    # Append log entry\n",
    "    log_df.write.format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(\"agriculture_data.pipeline_logs\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Create failure log dataframe\n",
    "    log_df = spark.createDataFrame(\n",
    "        [(pipeline_name, layer, table_name, \"FAILED\", None, None, None, str(e))],\n",
    "        schema=log_schema\n",
    "    )\n",
    "\n",
    "    log_df = log_df.withColumn(\"start_time\", current_timestamp()) \\\n",
    "                   .withColumn(\"end_time\", current_timestamp())\n",
    "\n",
    "    log_df.write.format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(\"agriculture_data.pipeline_logs\")\n",
    "\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6eb61be0-2425-446a-af91-3b6df787be13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Bronze Soil Health Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae27a88d-6728-453c-b3cb-3dbb54b4bd2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load raw soil health dataset\n",
    "soil_bronze_df = spark.table(\"bronze_soil_health\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d6cb500-fc26-45a0-92f7-883c5def9350",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Clean and Validate Soil Health Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa995cfc-ad30-4a41-93c3-8b5e536823b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply soil quality validation rules\n",
    "\n",
    "soil_clean_df = (\n",
    "    soil_bronze_df\n",
    "    # pH range is constrained to realistic\n",
    "    .filter(col(\"ph_level\").between(5.5, 8.5))\n",
    "    # Required fields present\n",
    "    .filter(\n",
    "        col(\"state\").isNotNull() &\n",
    "        col(\"district\").isNotNull()\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2e268a6-1fd1-45ae-a69c-410a8c7fac47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Deduplicate Soil Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76a46a23-350f-41ab-9e68-0cea4efef981",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Keep one soil health record per region\n",
    "soil_dedup_df = soil_clean_df.dropDuplicates([\"state\", \"district\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51e4a6af-1fad-420d-934a-a515b5823afc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Silver – Logging – soil_health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1ed42ac-64a0-4851-adcf-57a7366d7e65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Logging metadata\n",
    "pipeline_name = \"agriculture_pipeline\"\n",
    "layer = \"SILVER\"\n",
    "table_name = \"silver_soil_health\"\n",
    "\n",
    "try:\n",
    "    # Record count after soil cleaning\n",
    "    record_count = soil_dedup_df.count()\n",
    "\n",
    "    # Write silver soil table\n",
    "    soil_dedup_df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(table_name)\n",
    "\n",
    "    # Create success log dataframe\n",
    "    log_df = spark.createDataFrame(\n",
    "        [(pipeline_name, layer, table_name, \"SUCCESS\", record_count, None, None, None)],\n",
    "        schema=log_schema\n",
    "    )\n",
    "\n",
    "    log_df = log_df.withColumn(\"start_time\", current_timestamp()) \\\n",
    "                   .withColumn(\"end_time\", current_timestamp())\n",
    "\n",
    "    log_df.write.format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(\"agriculture_data.pipeline_logs\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Create failure log dataframe\n",
    "    log_df = spark.createDataFrame(\n",
    "        [(pipeline_name, layer, table_name, \"FAILED\", None, None, None, str(e))],\n",
    "        schema=log_schema\n",
    "    )\n",
    "\n",
    "    log_df = log_df.withColumn(\"start_time\", current_timestamp()) \\\n",
    "                   .withColumn(\"end_time\", current_timestamp())\n",
    "\n",
    "    log_df.write.format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(\"agriculture_data.pipeline_logs\")\n",
    "\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a21b0f0f-0c6a-4cd3-9cc0-65461a4347f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Bronze Market Prices Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b306d426-bfbb-417b-b88e-d0b205caeb0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load raw market prices data\n",
    "market_bronze_df = spark.table(\"bronze_market_prices\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f34bfd3-81d1-4362-978c-02c7addbc3c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Clean and Validate Market Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6ae3bd2-df5c-4492-9ae4-9d5a5053d26d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate market price records\n",
    "market_clean_df = (\n",
    "    market_bronze_df\n",
    "    # Valid price values\n",
    "    .filter(col(\"market_price_per_quintal\") > 0)\n",
    "    .filter(\n",
    "        col(\"crop\").isNotNull() &\n",
    "        col(\"year\").isNotNull()\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "862cdd38-fecc-42dd-9373-89a460c5d541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Standardize Crop Names in Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "453ec6e4-e5af-4030-ae10-838efc752f9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standardize crop names\n",
    "market_standardized_df = (\n",
    "    market_clean_df\n",
    "    # Normalize crop values\n",
    "    .withColumn(\"crop\", trim(lower(col(\"crop\"))))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3622d3c9-2ed0-4cfd-9263-804f0cbb5ca9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Silver – Logging – market_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a189f583-9830-41da-9fb5-1ede76b33511",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Logging metadata\n",
    "pipeline_name = \"agriculture_pipeline\"\n",
    "layer = \"SILVER\"\n",
    "table_name = \"silver_market_prices\"\n",
    "\n",
    "try:\n",
    "    # Record count after market price cleaning\n",
    "    record_count = market_standardized_df.count()\n",
    "\n",
    "    # Write silver market prices table\n",
    "    market_standardized_df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(table_name)\n",
    "\n",
    "    # Create success log dataframe\n",
    "    log_df = spark.createDataFrame(\n",
    "        [(pipeline_name, layer, table_name, \"SUCCESS\", record_count, None, None, None)],\n",
    "        schema=log_schema\n",
    "    )\n",
    "\n",
    "    log_df = log_df.withColumn(\"start_time\", current_timestamp()) \\\n",
    "                   .withColumn(\"end_time\", current_timestamp())\n",
    "\n",
    "    log_df.write.format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(\"agriculture_data.pipeline_logs\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Create failure log dataframe\n",
    "    log_df = spark.createDataFrame(\n",
    "        [(pipeline_name, layer, table_name, \"FAILED\", None, None, None, str(e))],\n",
    "        schema=log_schema\n",
    "    )\n",
    "\n",
    "    log_df = log_df.withColumn(\"start_time\", current_timestamp()) \\\n",
    "                   .withColumn(\"end_time\", current_timestamp())\n",
    "\n",
    "    log_df.write.format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(\"agriculture_data.pipeline_logs\")\n",
    "\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f6ae0ed-f8bd-4375-93d9-d12046bdebad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data Quality Validation Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d583cf25-4510-4cfe-8f47-252ff1e8dede",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records after cleaning: 10000\nRecords with null yield: 0\n"
     ]
    }
   ],
   "source": [
    "# Total records\n",
    "total_records = crop_enriched_df.count()\n",
    "\n",
    "# Null yield records\n",
    "null_yield_count = crop_enriched_df.filter(\n",
    "    col(\"yield_tonnes_per_hectare\").isNull()\n",
    ").count()\n",
    "\n",
    "# Print counts\n",
    "print(f\"Total records after cleaning: {total_records}\")\n",
    "print(f\"Records with null yield: {null_yield_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5062203471271728,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}