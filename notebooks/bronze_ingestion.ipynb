{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5f5821d-7fd9-4421-9c6d-6e1889d8d769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "BRONZE LAYER – RAW DATA INGESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c06898d-df62-4a61-a0e7-2609055b9291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.empty-table+json": {
       "directive_name": "NoDirective"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Create table for pipeline execution logs\n",
    "CREATE TABLE IF NOT EXISTS agriculture_data.pipeline_logs (\n",
    "    pipeline_name STRING,\n",
    "    layer STRING,\n",
    "    table_name STRING,\n",
    "    status STRING,\n",
    "    record_count LONG,\n",
    "    start_time TIMESTAMP,\n",
    "    end_time TIMESTAMP,\n",
    "    error_message STRING\n",
    ") USING DELTA;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df064c1b-0725-4ae7-b6b7-66dbc0828492",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.empty-table+json": {
       "directive_name": "NoDirective"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "USE CATALOG main;\n",
    "USE SCHEMA agriculture_data;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c261e956-12c4-4a6e-816b-c8d5d63d7ca0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use agriculture database\n",
    "spark.sql(\"USE agriculture_data\")\n",
    "\n",
    "# Read raw crop production CSV file\n",
    "df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"/Volumes/ashritha/default/agriculture/raw/crop_production.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e89bb9d8-b8b2-4462-b760-b494b42f48bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Bronze – Logging – crop_production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bb94185-fde2-48c1-9e82-5ddae3641cc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, LongType, TimestampType\n",
    ")\n",
    "from pyspark.sql.functions import current_timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a053d188-8d7b-4023-9cc2-9cfa3ce65125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "log_schema = StructType([\n",
    "    StructField(\"pipeline_name\", StringType(), True),\n",
    "    StructField(\"layer\", StringType(), True),\n",
    "    StructField(\"table_name\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"record_count\", LongType(), True),\n",
    "    StructField(\"start_time\", TimestampType(), True),\n",
    "    StructField(\"end_time\", TimestampType(), True),\n",
    "    StructField(\"error_message\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "111c061a-402d-4d61-8789-d7a6cd02c798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_name = \"agriculture_pipeline\"\n",
    "layer = \"BRONZE\"         \n",
    "table_name = \"bronze_crop_production\"\n",
    "\n",
    "try:\n",
    "    # Count records processed\n",
    "    record_count = df.count()\n",
    "\n",
    "    # Write data to target table\n",
    "    df.write.format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(table_name)\n",
    "\n",
    "    # Create success log dataframe with schema\n",
    "    log_df = spark.createDataFrame(\n",
    "        [(pipeline_name, layer, table_name, \"SUCCESS\", record_count, None, None, None)],\n",
    "        schema=log_schema\n",
    "    )\n",
    "\n",
    "    # Add timestamps\n",
    "    log_df = log_df \\\n",
    "        .withColumn(\"start_time\", current_timestamp()) \\\n",
    "        .withColumn(\"end_time\", current_timestamp())\n",
    "\n",
    "    # Append success log\n",
    "    log_df.write.format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(\"agriculture_data.pipeline_logs\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Create failure log dataframe with schema\n",
    "    log_df = spark.createDataFrame(\n",
    "        [(pipeline_name, layer, table_name, \"FAILED\", None, None, None, str(e))],\n",
    "        schema=log_schema\n",
    "    )\n",
    "\n",
    "    # Add timestamps\n",
    "    log_df = log_df \\\n",
    "        .withColumn(\"start_time\", current_timestamp()) \\\n",
    "        .withColumn(\"end_time\", current_timestamp())\n",
    "\n",
    "    # Append failure log\n",
    "    log_df.write.format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(\"agriculture_data.pipeline_logs\")\n",
    "\n",
    "    # Fail job for Airflow\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e46f22bb-188b-4acd-830e-0fc25f1df9c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use agriculture database\n",
    "spark.sql(\"USE agriculture_data\")\n",
    "\n",
    "# Read raw weather dataset\n",
    "df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"/Volumes/ashritha/default/agriculture/raw/weather_data.csv\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19c5063a-71a4-4333-84f4-f5f2934aa349",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Bronze – Logging – weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd0923b1-0569-4b6a-8568-152b22979f27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_name = \"agriculture_pipeline\"\n",
    "layer = \"BRONZE\"         \n",
    "table_name = \"bronze_weather\"\n",
    "\n",
    "try:\n",
    "    # Count records processed\n",
    "    record_count = df.count()\n",
    "\n",
    "    # Write data to target table\n",
    "    df.write.format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(table_name)\n",
    "\n",
    "    # Create success log dataframe with schema\n",
    "    log_df = spark.createDataFrame(\n",
    "        [(pipeline_name, layer, table_name, \"SUCCESS\", record_count, None, None, None)],\n",
    "        schema=log_schema\n",
    "    )\n",
    "\n",
    "    # Add timestamps\n",
    "    log_df = log_df \\\n",
    "        .withColumn(\"start_time\", current_timestamp()) \\\n",
    "        .withColumn(\"end_time\", current_timestamp())\n",
    "\n",
    "    # Append success log\n",
    "    log_df.write.format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(\"agriculture_data.pipeline_logs\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Create failure log dataframe with schema\n",
    "    log_df = spark.createDataFrame(\n",
    "        [(pipeline_name, layer, table_name, \"FAILED\", None, None, None, str(e))],\n",
    "        schema=log_schema\n",
    "    )\n",
    "\n",
    "    # Add timestamps\n",
    "    log_df = log_df \\\n",
    "        .withColumn(\"start_time\", current_timestamp()) \\\n",
    "        .withColumn(\"end_time\", current_timestamp())\n",
    "\n",
    "    # Append failure log\n",
    "    log_df.write.format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(\"agriculture_data.pipeline_logs\")\n",
    "\n",
    "    # Fail job for Airflow\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac40b1e5-2874-40b7-b239-dd50a765a4e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use agriculture database\n",
    "spark.sql(\"USE agriculture_data\")\n",
    "\n",
    "# Read raw soil health dataset\n",
    "df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"/Volumes/ashritha/default/agriculture/raw/soil_health.csv\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a280d5d0-ec83-4d97-a34b-d6059d6d02a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Bronze – Logging – soil_health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bbccd0c-7deb-4a95-98c2-fdb0bb17fac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_name = \"agriculture_pipeline\"\n",
    "layer = \"BRONZE\"         \n",
    "table_name = \"bronze_soil_health\"\n",
    "\n",
    "try:\n",
    "    # Count records processed\n",
    "    record_count = df.count()\n",
    "\n",
    "    # Write data to target table\n",
    "    df.write.format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(table_name)\n",
    "\n",
    "    # Create success log dataframe with schema\n",
    "    log_df = spark.createDataFrame(\n",
    "        [(pipeline_name, layer, table_name, \"SUCCESS\", record_count, None, None, None)],\n",
    "        schema=log_schema\n",
    "    )\n",
    "\n",
    "    # Add timestamps\n",
    "    log_df = log_df \\\n",
    "        .withColumn(\"start_time\", current_timestamp()) \\\n",
    "        .withColumn(\"end_time\", current_timestamp())\n",
    "\n",
    "    # Append success log\n",
    "    log_df.write.format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(\"agriculture_data.pipeline_logs\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Create failure log dataframe with schema\n",
    "    log_df = spark.createDataFrame(\n",
    "        [(pipeline_name, layer, table_name, \"FAILED\", None, None, None, str(e))],\n",
    "        schema=log_schema\n",
    "    )\n",
    "\n",
    "    # Add timestamps\n",
    "    log_df = log_df \\\n",
    "        .withColumn(\"start_time\", current_timestamp()) \\\n",
    "        .withColumn(\"end_time\", current_timestamp())\n",
    "\n",
    "    # Append failure log\n",
    "    log_df.write.format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(\"agriculture_data.pipeline_logs\")\n",
    "\n",
    "    # Fail job for Airflow\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bab79e6e-371f-4e0b-9dff-a4b70fd6248f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use agriculture database\n",
    "spark.sql(\"USE agriculture_data\")\n",
    "\n",
    "# Read raw market prices dataset\n",
    "df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"/Volumes/ashritha/default/agriculture/raw/market_prices.csv\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e93519b-bbd7-4006-8bae-404195256a40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Bronze – Logging – market_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cc1baaa-c36c-4472-8a6a-60d9b3c0a687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_name = \"agriculture_pipeline\"\n",
    "layer = \"BRONZE\"         \n",
    "table_name = \"bronze_market_prices\"\n",
    "\n",
    "try:\n",
    "    # Count records processed\n",
    "    record_count = df.count()\n",
    "\n",
    "    # Write data to target table\n",
    "    df.write.format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(table_name)\n",
    "\n",
    "    # Create success log dataframe with schema\n",
    "    log_df = spark.createDataFrame(\n",
    "        [(pipeline_name, layer, table_name, \"SUCCESS\", record_count, None, None, None)],\n",
    "        schema=log_schema\n",
    "    )\n",
    "\n",
    "    # Add timestamps\n",
    "    log_df = log_df \\\n",
    "        .withColumn(\"start_time\", current_timestamp()) \\\n",
    "        .withColumn(\"end_time\", current_timestamp())\n",
    "\n",
    "    # Append success log\n",
    "    log_df.write.format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(\"agriculture_data.pipeline_logs\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Create failure log dataframe with schema\n",
    "    log_df = spark.createDataFrame(\n",
    "        [(pipeline_name, layer, table_name, \"FAILED\", None, None, None, str(e))],\n",
    "        schema=log_schema\n",
    "    )\n",
    "\n",
    "    # Add timestamps\n",
    "    log_df = log_df \\\n",
    "        .withColumn(\"start_time\", current_timestamp()) \\\n",
    "        .withColumn(\"end_time\", current_timestamp())\n",
    "\n",
    "    # Append failure log\n",
    "    log_df.write.format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(\"agriculture_data.pipeline_logs\")\n",
    "\n",
    "    # Fail job for Airflow\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8297572662670127,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}